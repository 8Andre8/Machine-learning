{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301 0.388 0.531 0.538\n",
      "0.504 0.429 0.321 0.317\n",
      "0.195 0.183 0.148 0.145\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, in_features, hidden_size, n_classes, activation='tanh'):\n",
    "        self.in_features = in_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_classes = n_classes\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.Waa = self.init_weight_matrix(size=(self.hidden_size, self.hidden_size))\n",
    "        self.Wax = self.init_weight_matrix(size=(self.hidden_size, self.in_features))\n",
    "        self.Way = self.init_weight_matrix(size=(self.n_classes, self.hidden_size))\n",
    "        self.ba = self.init_weight_matrix(size=(self.hidden_size, 1))\n",
    "        self.by = self.init_weight_matrix(size=(self.n_classes, 1))\n",
    "        self.a0 = np.zeros(hidden_size)\n",
    "\n",
    "    def init_weight_matrix(self, size):\n",
    "        np.random.seed(1)\n",
    "        W = np.random.uniform(size=size)\n",
    "        return W\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a_t = self.a0\n",
    "        a = []\n",
    "        for i in range(x.shape[1]):\n",
    "            x_t = x[:, i].reshape(self.in_features, 1)\n",
    "            z_t_next = np.dot(self.Waa, a_t).reshape(-1, 1) + np.dot(self.Wax, x_t).reshape(-1, 1) + self.ba\n",
    "            a_t = self.tanh(z_t_next)\n",
    "            a.append(a_t)\n",
    "        a = np.array(a)[:, :, 0].T\n",
    "        y_hat = []\n",
    "        for i in range(x.shape[1]):\n",
    "            z_t = np.dot(self.Way, a[:, i].reshape(-1, 1)) + self.by\n",
    "            y_hat.append(self.softmax(z_t.T).T)\n",
    "        y_hat = np.array(y_hat)\n",
    "        return y_hat[:, :, 0].T\n",
    "    \n",
    "\n",
    "def read_matrix(n_rows, dtype=float):\n",
    "    return np.array([list(map(dtype, input().split())) for _ in range(n_rows)])\n",
    "\n",
    "def print_matrix(matrix):\n",
    "    for row in matrix:\n",
    "        print(' '.join(map(str, row)))\n",
    "\n",
    "def solution():\n",
    "    in_features, hidden_size, n_classes = 3, 2, 3\n",
    "    input_vectors = np.array([[0.0, -1.0, 2.0, 3.0],\n",
    "                              [-3.0, 0.0, 1.0, 4.0],\n",
    "                             [--2, 1, 2 , 3]])\n",
    "\n",
    "    rnn = RNN(in_features, hidden_size, n_classes)\n",
    "    output = rnn.forward(input_vectors).round(3)\n",
    "    print_matrix(output)\n",
    "\n",
    "solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0., -1.,  2.],\n",
      "        [-3.,  0.,  1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3187, -0.3320],\n",
       "          [-0.1620, -0.0676]]], grad_fn=<TransposeBackward1>),\n",
       " tensor([[[-0.1620, -0.0676]]], grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "inputs = torch.Tensor(np.array([[0.0, -1.0, 2.0],\n",
    "                              [-3.0, 0.0, 1.0]]))\n",
    "\n",
    "print(inputs)\n",
    "# Number of features used as input. (Number of columns)\n",
    "INPUT_SIZE = 3\n",
    "# Number of previous time stamps taken into account.\n",
    "SEQ_LENGTH = 2\n",
    "# Number of features in last hidden state ie. number of output time-\n",
    "# steps to predict.See image below for more clarity.\n",
    "HIDDEN_SIZE = 2\n",
    "# Number of stacked rnn layers.\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 1\n",
    "inputs = inputs.view(BATCH_SIZE, SEQ_LENGTH, INPUT_SIZE)\n",
    "rnn = nn.RNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, num_layers =NUM_LAYERS, batch_first=True)\n",
    "rnn.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
